{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project for ISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Analysis: Score: -57  Positive Tweets: 21   Negative Tweets: 78   Total Tweets: 99\n",
      "{'sadness': 15, 'joy': 79, 'fear': 0, 'disgust': 3, 'anger': 2}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Imports\n",
    "# ---------------------------------------------\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import tweepy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import creds\n",
    "# CSV Files\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "# Reddit\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import numpy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# config = configparser.RawConfigParser()\n",
    "# config.read('config.py')\n",
    "# ---------------------------------------------\n",
    "# IBM Credentials for emotion\n",
    "# ---------------------------------------------\n",
    "ibm_url  = \"https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/d6058b89-d39d-464c-a756-50658dd3124b\"\n",
    "ibm_api_key    = creds.IBM_KEY\n",
    "# print(ibm_api_key)\n",
    "# test = config.get(\"default\", \"IBM_KEY\")\n",
    "\n",
    "# print(test)\n",
    "# print(config.get(\"default\", \"IBM_KEY\"))\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Twitter API\n",
    "# ---------------------------------------------\n",
    "consumer_key = creds.CONSUMER_KEY\n",
    "consumer_secret = creds.CONSUMER_SECRET\n",
    "access_token = creds.ACCESS_TOKEN\n",
    "access_token_secret = creds.ACCESS_TOKEN_SECRET\n",
    "\n",
    "# Authenticating\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Loading the Positive and Negative Words\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Positive Words\n",
    "file = open('positive-words.txt', 'r')\n",
    "positive_words = file.read().splitlines() \n",
    "\n",
    "# Negative Words\n",
    "file = open('negative-words.txt', 'r')\n",
    "negative_words = file.read().splitlines() \n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Querying Tweets\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create tracking variables for tweet iteration\n",
    "tweets = []\n",
    "\n",
    "# Topic to be searched for\n",
    "topic = \"facebook\"\n",
    "\n",
    "# Get and store the 100 latest tweets\n",
    "all_tweets = api.search_tweets(q=f\"{topic} -filter:retweets\", lang=\"en\", count=100)\n",
    "all_tweets = [tweet.text.lower() for tweet in all_tweets]\n",
    "\n",
    "# 2-d array storing sentiment and emotion\n",
    "sentiment = numpy.empty((100, 2), numpy.str)\n",
    "\n",
    "# Keep track of score of sentiment for source\n",
    "num_pos_tweets = 0\n",
    "num_neg_tweets = 0\n",
    "num_neutral_tweets = 0\n",
    "count = 0\n",
    "emotionScore = {\n",
    "    'sadness':0,\n",
    "    'joy':0,\n",
    "    'fear':0,\n",
    "    'disgust':0,\n",
    "    'anger':0\n",
    "}\n",
    "\n",
    "# Iterate through last 100 tweets\n",
    "for tweet in all_tweets:\n",
    "    key_values = {'version': '2021-08-01', 'text': tweet, 'features':'sentiment,emotion'}\n",
    "    response = requests.get(ibm_url+\"/v1/analyze\", key_values, auth = ('apikey', ibm_api_key))\n",
    "    #print(response.json())\n",
    "    #  Make sure that tweet is able to be analyzed\n",
    "    if response.json()[\"language\"] == \"en\":\n",
    "        # see if sentiment is positive or negative\n",
    "        if response.json()[\"sentiment\"][\"document\"][\"label\"] == \"positive\":\n",
    "            num_pos_tweets += 1\n",
    "            sentiment[count][0] = \"positive\"\n",
    "        else:\n",
    "            num_neg_tweets += 1\n",
    "            sentiment[count][0] = \"negative\"\n",
    "\n",
    "        # determine strongest emotion and store it\n",
    "        sadness = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"sadness\"]\n",
    "        joy = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"joy\"]\n",
    "        fear = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"joy\"]\n",
    "        disgust = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"disgust\"]\n",
    "        anger = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"anger\"]\n",
    "        emotionMax = max(sadness, joy, fear, disgust, anger)\n",
    "        if emotionMax == sadness:\n",
    "            emotionScore['sadness'] = emotionScore['sadness'] + 1\n",
    "            sentiment[count][1] = 'sadness'\n",
    "        elif emotionMax == joy:\n",
    "            emotionScore['joy'] = emotionScore['joy'] + 1\n",
    "            sentiment[count][1] = 'joy'\n",
    "        elif emotionMax == fear:\n",
    "            emotionScore['fear'] = emotionScore['fear'] + 1\n",
    "            sentiment[count][1] = 'fear'\n",
    "        elif emotionMax == disgust:\n",
    "            emotionScore['disgust'] = emotionScore['disgust'] + 1\n",
    "            sentiment[count][1] = 'disgust'\n",
    "        else:\n",
    "            emotionScore['anger'] = emotionScore['anger'] + 1\n",
    "            sentiment[count][1] = 'anger'\n",
    "\n",
    "        count += 1\n",
    "\n",
    "# Once we are done analyzing the tweets, print overall score\n",
    "print(f\"{'Tweet Analysis:':<15} Score: {num_pos_tweets-num_neg_tweets:<4} Positive Tweets: {num_pos_tweets:<4} Negative Tweets: {num_neg_tweets:<4} Total Tweets: {count}\")\n",
    "print(emotionScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.5.0 of praw is outdated. Version 7.6.0 was released Tuesday May 10, 2022.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/88/l0rstcb91tn63mf9tdbd364r0000gn/T/ipykernel_47607/3632186039.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Save the comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0msources_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CNBC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredditScraping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnbcArticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0msources_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CNN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredditScraping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnnArticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0msources_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"The Verge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredditScraping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvergeArticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0msources_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IGN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredditScraping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignArticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/88/l0rstcb91tn63mf9tdbd364r0000gn/T/ipykernel_47607/3632186039.py\u001b[0m in \u001b[0;36mredditScraping\u001b[0;34m(redditAPI, url)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtopcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Loop through the top comments and grab the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtop_level_comment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/praw/models/reddit/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"\"\"Return the value of `attribute`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         raise AttributeError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0msubmission_listing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomment_listing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At most one of `data` and `json` is supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             return self._core.request(\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"api_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         return self._request_with_retries(\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mretry_strategy_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         response, saved_exception = self._make_request(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[1;32m    183\u001b[0m     ):\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             response = self._rate_limiter.call(\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_header_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \"\"\"\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/prawcore/rate_limit.py\u001b[0m in \u001b[0;36mdelay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Sleeping: {sleep_seconds:0.2f} seconds prior to call\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Querying Reddit\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Reddit API Key Data\n",
    "my_client_id = \"hO52jacJFobA_5P5KyIw8A\"\n",
    "my_client_secret = \"KHTg5c1aqtih_tj2qBjUdRM0GV9qiQ\"\n",
    "my_user_agent = \"Scraping ISA414\"\n",
    "\n",
    "# CNBC Article\n",
    "cnbcArticle = \"qht57x\"\n",
    "# CNN Article\n",
    "cnnArticle = \"quv6cr\"\n",
    "# Verge Article\n",
    "vergeArticle = \"qjbmtn\"\n",
    "# IGN  Article\n",
    "ignArticle = \"qmij5v\"\n",
    "# Business Insider Article\n",
    "biArticle = \"qhweln\"\n",
    "# Associated Press Article\n",
    "apArticle = \"qla1aw\"\n",
    "# NBC News Article\n",
    "nbcArticle = \"qht8mn\"\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = my_client_id,\n",
    "    client_secret = my_client_secret,\n",
    "    user_agent = my_user_agent,\n",
    ")\n",
    "\n",
    "# Define the reddit scraping method\n",
    "def redditScraping(redditAPI, url):\n",
    "\n",
    "    # Variables\n",
    "    count = 0\n",
    "    topcomments = []\n",
    "    submission = reddit.submission(url)\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    # Loop through the top comments and grab the text\n",
    "    for top_level_comment in submission.comments:\n",
    "        topcomments.append(top_level_comment.body)\n",
    "    \n",
    "    return topcomments\n",
    "\n",
    "\n",
    "# Sources\n",
    "companies = [\"CNBC\", \"CNN\", \"The Verge\", \"IGN\", \"Business Insider\", \"Associated Press\", \"NBC News\"]\n",
    "\n",
    "sources_dict = dict.fromkeys(companies, [])\n",
    "\n",
    "\n",
    "# Save the comments  \n",
    "sources_dict[\"CNBC\"].append(redditScraping(reddit, cnbcArticle))\n",
    "sources_dict[\"CNN\"].append(redditScraping(reddit, cnnArticle))\n",
    "sources_dict[\"The Verge\"].append(redditScraping(reddit, vergeArticle))\n",
    "sources_dict[\"IGN\"].append(redditScraping(reddit, ignArticle))\n",
    "sources_dict[\"Business Insider\"].append(redditScraping(reddit, biArticle))\n",
    "sources_dict[\"Associated Press\"].append(redditScraping(reddit, apArticle))\n",
    "sources_dict[\"NBC News\"].append(redditScraping(reddit, nbcArticle))\n",
    "\n",
    "# Print the Dict\n",
    "print(\"Data Stored!\")\n",
    "print(len(sources_dict[\"CNN\"][0][0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Reddit Sentiment & Emotion\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Keep track of score of sentiment for source\n",
    "num_pos_reddit = 0\n",
    "num_neg_reddit = 0\n",
    "num_neutral_reddit = 0\n",
    "count = 0\n",
    "ibmpos = 0\n",
    "ibmneg = 0\n",
    "emotionScore = {\n",
    "    'sadness':0,\n",
    "    'joy':0,\n",
    "    'fear':0,\n",
    "    'disgust':0,\n",
    "    'anger':0\n",
    "}\n",
    "\n",
    "# Iterate through the reddit data\n",
    "\n",
    "# for source in sources_dict:\n",
    "#     for data in source:\n",
    "#         key_values = {'version': '2021-08-01', 'text': data, 'features':'sentiment,emotion'}\n",
    "#         response = requests.get(ibm_url+\"/v1/analyze\", key_values, auth = ('apikey', ibm_api_key))\n",
    "        \n",
    "#         # make sure that tweet is analyzable\n",
    "#         if response.json()[\"language\"] == \"en\":\n",
    "#             # see if sentiment is positive or negative\n",
    "#             if response.json()[\"sentiment\"][\"document\"][\"label\"] == \"positive\":\n",
    "#                 num_pos_reddit += 1\n",
    "#             else:\n",
    "#                 num_neg_reddit += 1\n",
    "\n",
    "#             # determine strongest emotion\n",
    "#             sadness = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"sadness\"]\n",
    "#             joy = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"joy\"]\n",
    "#             fear = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"joy\"]\n",
    "#             disgust = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"disgust\"]\n",
    "#             anger = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"anger\"]\n",
    "#             emotionMax = max(sadness, joy, fear, disgust, anger)\n",
    "#             if emotionMax == sadness:\n",
    "#                 emotionScore['sadness'] = emotionScore['sadness'] + 1\n",
    "#             elif emotionMax == joy:\n",
    "#                 emotionScore['joy'] = emotionScore['joy'] + 1\n",
    "#             elif emotionMax == fear:\n",
    "#                 emotionScore['fear'] = emotionScore['fear'] + 1\n",
    "#             elif emotionMax == disgust:\n",
    "#                 emotionScore['disgust'] = emotionScore['disgust'] + 1\n",
    "#             else:\n",
    "#                 emotionScore['anger'] = emotionScore['anger'] + 1\n",
    "\n",
    "#             count += 1\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"{'Tweet Analysis:':<15} Score: {num_pos_reddit-num_neg_reddit:<4} Positive Tweets: {num_pos_reddit:<4} Negative Tweets: {num_neg_reddit:<4} Total Tweets: {count}\")\n",
    "# print(emotionScore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Outputting Data\n",
    "# ---------------------------------------------\n",
    "\n",
    "# --------------------------------------------- Twitter\n",
    "\n",
    "output = [\"Test1\", \"Test2\"]\n",
    "# File name for the data\n",
    "twitterFile = 'twitter_data.csv'\n",
    "\n",
    "# Delete current CSV file, if it exists\n",
    "if(os.path.exists(twitterFile) and os.path.isfile(twitterFile)):\n",
    "  os.remove(twitterFile)\n",
    "\n",
    "# Writing to csv File\n",
    "header = [\"Tweet\", \"Sentiment\", \"Emotion\"]\n",
    "\n",
    "# Open the CSV to write to\n",
    "with open(twitterFile, 'w', encoding='UTF8', newline='') as f:\n",
    "    # Create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # Create the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Write the data\n",
    "    for i in range(0, len(all_tweets)):\n",
    "      writer.writerow([all_tweets[i], sentiment[i][0], sentiment[i][1]])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------- Reddit\n",
    "\n",
    "# File name for the data\n",
    "redditFile = 'reddit_data.csv'\n",
    "\n",
    "# Delete current CSV file, if it exists\n",
    "if(os.path.exists(redditFile) and os.path.isfile(redditFile)):\n",
    "  os.remove(redditFile)\n",
    "\n",
    "\n",
    "# Writing to CSV file\n",
    "header = [\"News Source\", \"Tweet\"]\n",
    "\n",
    "# Open the CSV to write to\n",
    "with open(redditFile, 'w', encoding='UTF8', newline='') as f:\n",
    "    # Create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # Create the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Write the data\n",
    "    for key, val in sources_dict.items():\n",
    "      for i in val: \n",
    "        writer.writerow([key, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Analyzing Data\n",
    "# ---------------------------------------------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
